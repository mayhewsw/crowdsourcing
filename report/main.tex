\documentclass[11pt]{article}
%\usepackage{aistats2014}

\usepackage{fullpage,color}
\usepackage{graphicx,amsmath,framed} % jeffe,handout,handout  ,
\usepackage{algorithm2e,framed}
% ge[utf8]{inputenc}		% Allow some non-ASCII Unicode in source
\usepackage{amsfonts,amssymb,hyperref}
\usepackage{framed}
\usepackage{natbib}
\usepackage{wrapfig,multirow}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{answer}{Answer}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\linespread{1.2}
\newif\iflogvar
\logvartrue

% If your paper is accepted, change the options for the package
% aistats2014 as follows:
%
%\usepackage[accepted]{aistats2014}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

%\twocolumn[

%\aistatstitle{ Clustering With Side Information: \\ From Probabilistic to Deterministic Algorithms }

%\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

%\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\title{Crowdsourcing Algorithms: \\Summary and Comparisons}

\maketitle

\begin{abstract}
Sid
\end{abstract}

\section{Introduction}
[TODO]

\textbf{Problem definition and notation: }
Define: 

$t_i \in \{ +1, -1 \} $: variables of interest to be predicted

$p_j \in [0,1]$: Latent variable 

$A_{ij}\in \{+1, -1\}$: Observations 

$
g_{ij}(t_i,p_j) = p_j^ {  \mathbf{I} \lbrace A_{ij} = t_i \rbrace } \times (1-p_j)^{ \mathbf{I} \lbrace A_{ij} \neq t_i \rbrace}
$


\section{Methods}
\subsection{Belief Propagation}
[TODO: explanation about derivations]

$$
\psi_{ij}(t_i, p_j) = p_j \mathbf{I} \left\lbrace  A_{ij}=  t_i  \right\rbrace +  (1-p_j ) \mathbf{I} \left\lbrace  A_{ij} \neq  t_i  \right\rbrace
$$

$$
\mathbb{F}_{mf}(b) = \sum_{(i, j) \in E} \sum_{t_i, p_j} b_i^L(t_i) b_j^R(p_j) \psi_{ij}(t_i, p_j) + \sum_{i \in V_L} \sum_{t_i} b_i^L(t_i)  \ln b_i^L(t_i) +  \sum_{j \in V_R} \sum_{p_j} b_j^R(p_j)  \ln b_j^R(p_j)
$$
$$
\mathcal{L} = \mathbb{F}_{mf}(b) - \sum_{i \in V_L} \lambda_i^L \left\lbrace  \sum_{t_i} b_i^L(t_i)  - 1  \right\rbrace - \sum_{j \in V_R} \lambda_j^R \left\lbrace  \sum_{p_j} b_j^R(p_j)  - 1  \right\rbrace
$$

$$
\begin{cases}
\frac{\partial \mathcal{L}}{\partial b_i^L} = \sum_{j \in V_R} \sum_{p_j}  b_j^R(p_j) \psi_{ij}(t_i, p_j) +  b_i^L(t_i) +1 - \lambda_i^L  = 0 \\ 
\frac{\partial \mathcal{L}}{\partial b_j^R} = \sum_{i \in V_L} \sum_{t_i}  b_i^L(t_i) \psi_{ij}(t_i, p_j) +  b_j^R(p_j) +1 - \lambda_j^R  = 0  
\end{cases}
$$

$$
\Rightarrow  
\begin{cases}
b_i^L(t_i) \propto  \exp \left\lbrace  - \sum_{j \in V_R} \sum_{p_j}  b_j^R(p_j) \psi_{ij}(t_i, p_j) \right\rbrace \\ 
b_j^R(p_j) \propto \exp \left\lbrace  - \sum_{i \in V_L} \sum_{t_i}  b_i^L(t_i) \psi_{ij}(t_i, p_j)   \right\rbrace 
\end{cases}
$$

\subsection{Expectation-Maximization}
Consider having the following prior on each $p_j$: 
$$
c + (1-c)Z_j, \quad \text{ where } Z_j \sim \text{Beta}(\alpha_j, \beta_j) 
$$
The full likelihood: 
$L(p; t) = \prod_{i,j} g_{ij}(t_i,p_j) \prod_j \left( c+ \frac{1-c}{B(\alpha_j, \beta_j)} p_j^{\alpha_j-1} (1-p_j)^{\beta_j-1}  \right) $

The likelihood for worker $j$: 
\begin{align*}
L_j(p; t) &= \prod_{i} g_{ij}(t_i,p_j) \left( c+ \frac{1-c}{B(\alpha_j, \beta_j)} p_j^{\alpha_j-1} (1-p_j)^{\beta_j-1}  \right) \\
&= \frac{1-c}{B(\alpha_j, \beta_j)}p_j^{ \alpha_j-1 + \sum_i \mathbf{I} \lbrace A_{ij} = t_i \rbrace }  (1-p_j)^{ \beta_j-1 + \sum_i \mathbf{I} \lbrace A_{ij} \neq t_i \rbrace } + cp_j^{\sum_i \mathbf{I} \lbrace A_{ij} = t_i \rbrace } (1-p_j)^{  \sum_i \mathbf{I} \lbrace A_{ij} \neq t_i \rbrace } 
\end{align*}

\textbf{E-step:} 

Suppose $t_i \sim q_i$. Then:
\begin{align*}
Q_j(p) =\mathbb{E}_{t}\left[ \ln L_j(p; t) \right] = \sum_tq(t) \Bigg[&  
\ln \frac{1-c}{B(\alpha_j, \beta_j)} + \left(  { \alpha_j-1 + \sum_i \mathbf{I} \lbrace A_{ij} = t_i \rbrace } \right)  \ln p_j  \\ &+ \left( { \beta_j-1 + \sum_i \mathbf{I} \lbrace A_{ij} \neq t_i \rbrace } \right) \ln (1-p_j)  \\ 
&+ \ln c + \left( \sum_i \mathbf{I} \lbrace A_{ij} = t_i \rbrace \right) \ln p_j + \left( {  \sum_i \mathbf{I} \lbrace A_{ij} \neq t_i \rbrace } \right) \ln (1-p_j) \Bigg], 
\end{align*}
which can be simplified to
\begin{align*}
Q_j(p) =   
\ln \frac{1-c}{B(\alpha_j, \beta_j)}  + \ln c +  \left(   \alpha_j-1 \right)  \ln p_j  + \left( \beta_j - 1  \right) \ln (1-p_j) +  \Bigg[&  
\ln p_j \sum_i q(A_{ij}) +  \ln (1-p_j) \sum_i q(-A_{ij}) \Bigg], 
\end{align*}

where 
$$
q_i(t_i) = \frac{ \prod_{j} g_{ij}(t_i, p_j) }{ \sum_{t_i} \prod_{j} g_{ij}(t_i, p_j)   }  \quad \quad (1)
$$

\textbf{M-step: }
Taking derivative with respect to $p_j$: 
$$
\frac{\partial Q_j}{\partial p_j} = \frac{\alpha_j - 1 + \sum_jq(A_{ij})}{p_j} + \frac{\beta_j-1  + \sum_j q(-A_{ij}) }{1-p_j} = 0
$$
which results in 
$$
p_j = \frac{ \alpha_j - 1 + \sum_jq(A_{ij}) }{ \alpha_j + \beta_j-2 + \sum_jq(A_{ij})  + \sum_j q(-A_{ij})  }
$$




\section{Experiments}
[TODO]

\section*{Acknowledgment}
The authors would like to thank Ke Jiang for providing the the data used in their work. We also thank Shyam Upadhyay and Daphne Tsatsoulis for helpful comments on the draft. 

\bibliographystyle{ba}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{ref}


\end{document}
